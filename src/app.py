"""
åƒåœ¾éƒµä»¶åˆ†é¡ Streamlit ç¶²é æ‡‰ç”¨
"""

import streamlit as st
import joblib
import os
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score
import altair as alt
from PIL import Image
from preprocessing import preprocess_text

def load_models():
    """
    è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹å’Œå‘é‡åŒ–å™¨
    """
    try:
        # å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„çš„çµ•å°è·¯å¾‘
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        models_dir = os.path.join(project_root, 'models')
        
        # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = os.path.join(models_dir, 'model.pkl')
        vectorizer_path = os.path.join(models_dir, 'vectorizer.pkl')
        
        if not os.path.exists(model_path) or not os.path.exists(vectorizer_path):
            raise FileNotFoundError("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è¼‰å…¥æ¨¡å‹å’Œå‘é‡åŒ–å™¨
        model = joblib.load(model_path)
        vectorizer = joblib.load(vectorizer_path)
        return model, vectorizer
    except Exception as e:
        raise Exception(f"è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

def predict_spam(text, model, vectorizer):
    """
    é æ¸¬æ–‡æœ¬æ˜¯å¦ç‚ºåƒåœ¾éƒµä»¶
    
    Args:
        text (str): è¼¸å…¥æ–‡æœ¬
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        vectorizer: TF-IDF å‘é‡åŒ–å™¨
        
    Returns:
        float: åƒåœ¾éƒµä»¶çš„æ©Ÿç‡
    """
    # é è™•ç†æ–‡æœ¬
    text_clean = preprocess_text(text)
    
    # ç‰¹å¾µæå–
    features = vectorizer.transform([text_clean])
    
    # é æ¸¬
    proba = model.predict_proba(features)[0]
    return proba[1]  # è¿”å›æ˜¯åƒåœ¾éƒµä»¶çš„æ©Ÿç‡

def load_config():
    """Load model configuration"""
    try:
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(project_root, 'models', 'config.json')
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                return json.load(f)
        return None
    except Exception:
        return None

@st.cache_data
def load_full_dataset():
    """
    Loads the local dataset and caches it.
    """
    # Construct path relative to the project root
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    dataset_path = os.path.join(project_root, 'dataset', 'sms_spam_no_header.csv')
    df = pd.read_csv(dataset_path, names=['label', 'message'])
    df['label_num'] = (df['label'] == 'spam').astype(int)
    return df

def main():
    st.title("Spam Email Detection System ğŸš«âœ‰ï¸")
    st.write("This system uses machine learning to identify spam emails. Enter email content below for analysis.")
    
    # Load configuration
    config = load_config()
    
    # Sidebar configuration
    st.sidebar.header("Model Configuration")
    
    # Model parameters
    threshold = st.sidebar.slider(
        "Decision Threshold",
        min_value=0.1,
        max_value=0.9,
        value=0.5,
        step=0.01,
        help="Probability threshold for classifying an email as spam"
    )

    # Allow user to adjust test size and random seed and top-N tokens
    default_test_size = config['test_size'] if config and 'test_size' in config else 0.2
    default_seed = config['random_seed'] if config and 'random_seed' in config else 42

    test_size = st.sidebar.slider("Test Size", min_value=0.05, max_value=0.5, value=float(default_test_size), step=0.01, format="%.2f")
    random_seed = st.sidebar.number_input("Random Seed", min_value=0, value=int(default_seed))
    top_n = st.sidebar.number_input("Top-N Tokens", min_value=5, max_value=100, value=15)
    
    if config:
        st.sidebar.markdown("### Model Settings")
        st.sidebar.write(f"Test Size: {config['test_size']}")
        st.sidebar.write(f"Random Seed: {config['random_seed']}")
        
        st.sidebar.markdown("### Current Performance")
        st.sidebar.write(f"AUC Score: {config['model_performance']['auc_score']:.3f}")
        st.sidebar.write(f"Avg Precision: {config['model_performance']['avg_precision']:.3f}")
    
    # æ·»åŠ é ç±¤
    tab1, tab2 = st.tabs(["Prediction", "Model Analysis"])
    
    with tab1:
        # è¼‰å…¥æ¨¡å‹
        try:
            model, vectorizer = load_models()
        except Exception as e:
            # æ›´è©³ç´°çš„éŒ¯èª¤è¨ºæ–·ï¼Œå¹«åŠ©åœ¨é ç«¯ï¼ˆä¾‹å¦‚ Streamlit Cloudï¼‰æ’æŸ¥å•é¡Œ
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            models_dir = os.path.join(project_root, 'models')
            info_lines = []
            info_lines.append(f"è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”Ÿä¾‹å¤–: {e}")
            try:
                if os.path.exists(models_dir):
                    info_lines.append(f"models è³‡æ–™å¤¾ä½æ–¼: {models_dir}")
                    for fname in sorted(os.listdir(models_dir)):
                        fpath = os.path.join(models_dir, fname)
                        try:
                            size = os.path.getsize(fpath)
                        except Exception:
                            size = 'NA'
                        info_lines.append(f" - {fname} (size={size})")
                else:
                    info_lines.append("models è³‡æ–™å¤¾ä¸å­˜åœ¨æ–¼ repo æ ¹ç›®éŒ„")
            except Exception as _:
                info_lines.append("ç„¡æ³•åˆ—å‡º models å…§å®¹ï¼ˆæ¬Šé™æˆ–å…¶ä»–éŒ¯èª¤ï¼‰")

            st.error("éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥æ¨¡å‹ã€‚è«‹ç¢ºä¿å·²ç¶“é‹è¡Œé train_model.py è¨“ç·´æ¨¡å‹ï¼Œæˆ–å·²å°‡ models/ è³‡æ–™å¤¾æ¨é€è‡³é ç«¯ repoã€‚")
            with st.expander("è¼‰å…¥æ¨¡å‹è¨ºæ–·è³‡è¨Š"):
                for line in info_lines:
                    st.write(line)

            st.stop()
        
        # æ–‡æœ¬è¼¸å…¥
        text = st.text_area(
            "è«‹è¼¸å…¥éƒµä»¶å…§å®¹ï¼š",
            height=200,
            placeholder="åœ¨æ­¤è¼¸å…¥éƒµä»¶å…§å®¹..."
        )
        
        if st.button("åˆ†æ"):
            if not text.strip():
                st.warning("è«‹è¼¸å…¥éƒµä»¶å…§å®¹")
            else:
                # é€²è¡Œé æ¸¬
                spam_prob = predict_spam(text, model, vectorizer)
                
                # é¡¯ç¤ºçµæœ
                st.subheader("åˆ†æçµæœ")
                
                # ä½¿ç”¨é€²åº¦æ¢é¡¯ç¤ºé æ¸¬æ©Ÿç‡
                st.progress(spam_prob)
                
                # é¡¯ç¤ºé æ¸¬çµæœ
                if spam_prob > 0.5:
                    st.error(f"âš ï¸ é€™å¯èƒ½æ˜¯åƒåœ¾éƒµä»¶ (ä¿¡å¿ƒåº¦: {spam_prob:.1%})")
                else:
                    st.success(f"âœ… é€™å¯èƒ½æ˜¯æ­£å¸¸éƒµä»¶ (ä¿¡å¿ƒåº¦: {1-spam_prob:.1%})")
                
                # é¡¯ç¤ºè©³ç´°ä¿¡æ¯
                with st.expander("æŸ¥çœ‹è©³ç´°åˆ†æ"):
                    st.write(f"åƒåœ¾éƒµä»¶æ©Ÿç‡ï¼š{spam_prob:.2%}")
                    st.write(f"æ­£å¸¸éƒµä»¶æ©Ÿç‡ï¼š{1-spam_prob:.2%}")
                    
                    # é¡¯ç¤ºé è™•ç†å¾Œçš„æ–‡æœ¬
                    st.write("é è™•ç†å¾Œçš„æ–‡æœ¬ï¼š")
                    st.code(preprocess_text(text))

    with tab2:
        st.header("Model Analysis")

        # Show data distribution
        st.subheader("1. Data Distribution")
        def _show_image_safe(relpath, caption=None):
            p = os.path.join(os.getcwd(), relpath)
            if os.path.exists(p):
                try:
                    img = Image.open(p)
                    st.image(img, caption=caption)
                except Exception as e:
                    st.error(f"Failed to open image {relpath}: {e}")
            else:
                st.error(f"Cannot find image: {relpath}")

        _show_image_safe("visualizations/distribution.png")

        # Load dataset and compute dynamic metrics (without retraining the model)
        # We'll re-split using the selected test_size and random_seed, then evaluate the loaded model on that test set.
        try:
            full_df = load_full_dataset()

            # split according to sidebar inputs
            X_train, X_test, y_train, y_test = train_test_split(
                full_df['message'], full_df['label_num'], test_size=float(test_size), random_state=int(random_seed)
            )

            # preprocess and vectorize using loaded vectorizer
            X_test_pre = X_test.apply(preprocess_text)
            X_test_features = vectorizer.transform(X_test_pre)

            y_prob = model.predict_proba(X_test_features)
            y_pred = (y_prob[:, 1] >= threshold).astype(int)

            # compute metrics for dynamic display
            fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])
            roc_auc = auc(fpr, tpr)
            precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob[:, 1])
            avg_precision = average_precision_score(y_test, y_prob[:, 1])
            cm = confusion_matrix(y_test, y_pred)

            # threshold metrics table
            thresholds = np.round(np.arange(0.0, 1.01, 0.01), 2)
            rows = []
            for th in thresholds:
                yp = (y_prob[:, 1] >= th).astype(int)
                tp = np.sum((y_test == 1) & (yp == 1))
                fp = np.sum((y_test == 0) & (yp == 1))
                fn = np.sum((y_test == 1) & (yp == 0))
                tn = np.sum((y_test == 0) & (yp == 0))
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                accuracy = (tp + tn) / len(y_test)
                rows.append({'threshold': th, 'precision': precision, 'recall': recall, 'f1_score': f1, 'accuracy': accuracy})

            threshold_metrics_df = pd.DataFrame(rows)

        except Exception as e:
            st.error(f"Failed to compute dynamic metrics: {e}")
            full_df = None
            threshold_metrics_df = None

    # Top Tokens (interactive)
        st.subheader("2. Top Tokens Analysis (interactive)")
        try:
            toks_path = os.path.join('visualizations', 'top_tokens.csv')
            toks_df = pd.read_csv(toks_path)
            # allow selecting which class to view
            token_view = st.selectbox('Token view', options=['spam', 'ham', 'total'], index=0)
            if token_view == 'spam':
                plot_df = toks_df[['token', 'spam_count']].rename(columns={'spam_count': 'count'})
            elif token_view == 'ham':
                plot_df = toks_df[['token', 'ham_count']].rename(columns={'ham_count': 'count'})
            else:
                plot_df = toks_df[['token', 'total_count']].rename(columns={'total_count': 'count'})

            plot_df = plot_df.nlargest(int(top_n), 'count')[['token', 'count']]

            chart = alt.Chart(plot_df).mark_bar().encode(
                x=alt.X('count:Q'),
                y=alt.Y('token:N', sort='-x'),
                color=alt.Color('token:N', legend=None)
            ).properties(width=700, height=30 * min(len(plot_df), 30))
            st.altair_chart(chart, width='stretch')
            with st.expander('Top tokens data'):
                st.dataframe(plot_df.reset_index(drop=True))
        except Exception as e:
            st.error(f"Cannot load top tokens data: {e}")
        
        # Show threshold analysis (table) â€” dynamic using current sidebar Test Size/Seed/Threshold
        st.subheader("3. Threshold Analysis (table)")
        if threshold_metrics_df is not None:
            st.dataframe(threshold_metrics_df.style.format({
                'threshold': '{:.2f}',
                'precision': '{:.3f}',
                'recall': '{:.3f}',
                'f1_score': '{:.3f}',
                'accuracy': '{:.3f}'
            }))

            sel_thresh = st.slider(
                'Select Threshold to view row',
                min_value=float(threshold_metrics_df.threshold.min()),
                max_value=float(threshold_metrics_df.threshold.max()),
                value=float(threshold),
                step=0.01
            )
            row = threshold_metrics_df.iloc[(threshold_metrics_df['threshold'] - sel_thresh).abs().argsort()[:1]]
            st.markdown('**Metrics at selected threshold**')
            st.table(row.reset_index(drop=True).round(3))
        else:
            st.error('Threshold metrics unavailable')
        
        # Show ROC and PR curves
        st.subheader("4. Model Performance Curves")
        col1, col2 = st.columns(2)
        with col1:
            # Draw dynamic ROC curve using computed fpr/tpr
            try:
                roc_df = pd.DataFrame({'fpr': fpr, 'tpr': tpr})
                roc_chart = alt.Chart(roc_df).mark_line().encode(x='fpr', y='tpr')
                st.altair_chart(roc_chart.properties(height=300, width=400), width='stretch')
                st.write(f'AUC = {roc_auc:.3f}')
            except Exception:
                st.error("Cannot compute ROC curve")
        with col2:
            try:
                pr_df = pd.DataFrame({'recall': recall_vals, 'precision': precision_vals})
                pr_chart = alt.Chart(pr_df).mark_line().encode(x='recall', y='precision')
                st.altair_chart(pr_chart.properties(height=300, width=400), width='stretch')
                st.write(f'Average precision = {avg_precision:.3f}')
            except Exception:
                st.error("Cannot compute Precision-Recall curve")

        # Show confusion matrix
        st.subheader("5. Confusion Matrix")
        try:
            st.image("visualizations/confusion_matrix.png")
        except Exception:
            st.error("Cannot load confusion matrix")

    # æ·»åŠ èªªæ˜
    with st.sidebar:
        st.subheader("é—œæ–¼")
        st.write("""
        æ­¤æ‡‰ç”¨ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’ä¾†è­˜åˆ¥åƒåœ¾éƒµä»¶ã€‚
        
        ä½¿ç”¨çš„æŠ€è¡“ï¼š
        - æ”¯æŒå‘é‡æ©Ÿ (SVM)
        - TF-IDF æ–‡æœ¬å‘é‡åŒ–
        - NLTK æ–‡æœ¬é è™•ç†
        """)
        
        st.subheader("ä½¿ç”¨èªªæ˜")
        st.write("""
        1. åœ¨æ–‡æœ¬æ¡†ä¸­è¼¸å…¥éƒµä»¶å…§å®¹
        2. é»æ“Šã€Œåˆ†æã€æŒ‰éˆ•
        3. æŸ¥çœ‹é æ¸¬çµæœå’Œè©³ç´°åˆ†æ
        """)

if __name__ == "__main__":
    main()